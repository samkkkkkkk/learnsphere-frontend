"""
RAG ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ëª¨ë“ˆ

UniTask ê³µì‹ Repositoryì™€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ Qdrant ë²¡í„° DBì— ì €ì¥
ê°•ì˜ ìƒì„± ì‹œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•¨
"""

import os
import glob
import argparse
from typing import List, Dict, Tuple
from pathlib import Path

import openai
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv

from config import (
    get_subject_paths, 
    get_qdrant_collection_name,
    RAG_CONFIG, 
    GENERATION_CONFIG,
    BASE_DIR
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv(os.path.join(BASE_DIR, '..', '.env'))

class VectorBuilder:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• í´ë˜ìŠ¤"""
    
    def __init__(self, subject: str):
        self.subject = subject
        self.collection_name = get_qdrant_collection_name(subject)
        self.paths = get_subject_paths(subject)
        self.data_path = self.paths["data_dir"]
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        qdrant_url = os.getenv("QDRANT_URL", "http://localhost:6333")
        self.qdrant_client = QdrantClient(url=qdrant_url)
        
        # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=RAG_CONFIG["chunk_size"],
            chunk_overlap=RAG_CONFIG["chunk_overlap"],
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        
    def collect_source_files(self) -> List[Tuple[str, str]]:
        """ì†ŒìŠ¤ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤"""
        print(f"ğŸ“ ë°ì´í„° ê²½ë¡œì—ì„œ íŒŒì¼ ìˆ˜ì§‘ ì¤‘: {self.data_path}")
        
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f"ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_path}")
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í™•ì¥ì
        file_patterns = [
            "**/*.cs",      # C# ì†ŒìŠ¤ ì½”ë“œ
            "**/*.md",      # ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ
            "**/*.txt",     # í…ìŠ¤íŠ¸ íŒŒì¼
            "**/*.json"     # JSON ì„¤ì • íŒŒì¼
        ]
        
        files_content = []
        
        for pattern in file_patterns:
            file_paths = glob.glob(os.path.join(self.data_path, pattern), recursive=True)
            
            for file_path in file_paths:
                # ì œì™¸í•  íŒŒì¼ë“¤ (ë°”ì´ë„ˆë¦¬, ë©”íƒ€íŒŒì¼ ë“±)
                if any(exclude in file_path.lower() for exclude in [
                    '.meta', '.dll', '.exe', '.bin', 'packages-lock.json'
                ]):
                    continue
                    
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        
                    # ë¹ˆ íŒŒì¼ì´ë‚˜ ë„ˆë¬´ ì‘ì€ íŒŒì¼ ì œì™¸
                    if len(content.strip()) < 50:
                        continue
                        
                    # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    relative_path = os.path.relpath(file_path, self.data_path)
                    files_content.append((relative_path, content))
                    
                except Exception as e:
                    print(f"âš ï¸  íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_path} - {e}")
                    continue
        
        print(f"âœ… ì´ {len(files_content)}ê°œ íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ")
        return files_content
    
    def create_chunks(self, files_content: List[Tuple[str, str]]) -> List[Dict]:
        """íŒŒì¼ ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤"""
        print("ğŸ”„ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‘ì—… ì¤‘...")
        
        all_chunks = []
        
        for file_path, content in files_content:
            # í…ìŠ¤íŠ¸ ë¶„í• 
            chunks = self.text_splitter.split_text(content)
            
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "id": f"{file_path}_{i}",
                    "text": chunk,
                    "metadata": {
                        "file_path": file_path,
                        "chunk_index": i,
                        "subject": self.subject,
                        "file_type": os.path.splitext(file_path)[1]
                    }
                }
                all_chunks.append(chunk_data)
        
        print(f"âœ… ì´ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„± ì™„ë£Œ")
        return all_chunks
    
    def generate_embeddings(self, chunks: List[Dict]) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ì²­í¬ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤"""
        print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬ (API í•œê³„ ê³ ë ¤)
        batch_size = 100
        embedded_chunks = []
        
        for i in range(0, len(chunks), batch_size):
            batch = chunks[i:i + batch_size]
            texts = [chunk["text"] for chunk in batch]
            
            try:
                # OpenAI Embedding API í˜¸ì¶œ
                response = self.openai_client.embeddings.create(
                    model=RAG_CONFIG["embedding_model"],
                    input=texts
                )
                
                # ì‘ë‹µì—ì„œ ì„ë² ë”© ì¶”ì¶œ
                embeddings = [data.embedding for data in response.data]
                
                # ì²­í¬ì— ì„ë² ë”© ì¶”ê°€
                for j, chunk in enumerate(batch):
                    chunk["vector"] = embeddings[j]
                    embedded_chunks.append(chunk)
                
                print(f"âœ… ë°°ì¹˜ {i//batch_size + 1}/{(len(chunks)-1)//batch_size + 1} ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ (ë°°ì¹˜ {i//batch_size + 1}): {e}")
                raise
        
        print(f"âœ… ì´ {len(embedded_chunks)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        return embedded_chunks
    
    def setup_qdrant_collection(self):
        """Qdrant ì»¬ë ‰ì…˜ì„ ì„¤ì •í•©ë‹ˆë‹¤"""
        print(f"ğŸ”„ Qdrant ì»¬ë ‰ì…˜ ì„¤ì • ì¤‘: {self.collection_name}")
        
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ì´ ìˆìœ¼ë©´ ì‚­ì œ
        try:
            self.qdrant_client.delete_collection(self.collection_name)
            print(f"ğŸ—‘ï¸  ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {self.collection_name}")
        except:
            pass
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        self.qdrant_client.create_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(
                size=RAG_CONFIG["vector_dimension"],
                distance=Distance.COSINE
            )
        )
        
        print(f"âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ: {self.collection_name}")
    
    def store_vectors(self, embedded_chunks: List[Dict]):
        """ë²¡í„°ë“¤ì„ Qdrantì— ì €ì¥í•©ë‹ˆë‹¤"""
        print("ğŸ”„ ë²¡í„° ì €ì¥ ì¤‘...")
        
        # í¬ì¸íŠ¸ êµ¬ì¡° ìƒì„±
        points = []
        for i, chunk in enumerate(embedded_chunks):
            point = PointStruct(
                id=i,
                vector=chunk["vector"],
                payload={
                    "text": chunk["text"],
                    "file_path": chunk["metadata"]["file_path"],
                    "chunk_index": chunk["metadata"]["chunk_index"],
                    "subject": chunk["metadata"]["subject"],
                    "file_type": chunk["metadata"]["file_type"]
                }
            )
            points.append(point)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì €ì¥
        batch_size = 100
        for i in range(0, len(points), batch_size):
            batch = points[i:i + batch_size]
            
            try:
                self.qdrant_client.upsert(
                    collection_name=self.collection_name,
                    points=batch
                )
                print(f"âœ… ë°°ì¹˜ {i//batch_size + 1}/{(len(points)-1)//batch_size + 1} ì €ì¥ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ ë²¡í„° ì €ì¥ ì‹¤íŒ¨ (ë°°ì¹˜ {i//batch_size + 1}): {e}")
                raise
        
        print(f"âœ… ì´ {len(points)}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ")
    
    def build_vector_db(self):
        """ì „ì²´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• í”„ë¡œì„¸ìŠ¤"""
        print(f"ğŸš€ {self.subject} ë²¡í„° DB êµ¬ì¶• ì‹œì‘!")
        print("=" * 50)
        
        try:
            # 1. ì†ŒìŠ¤ íŒŒì¼ ìˆ˜ì§‘
            files_content = self.collect_source_files()
            
            # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
            chunks = self.create_chunks(files_content)
            
            # 3. ì„ë² ë”© ìƒì„±
            embedded_chunks = self.generate_embeddings(chunks)
            
            # 4. Qdrant ì»¬ë ‰ì…˜ ì„¤ì •
            self.setup_qdrant_collection()
            
            # 5. ë²¡í„° ì €ì¥
            self.store_vectors(embedded_chunks)
            
            print("=" * 50)
            print(f"ğŸ‰ {self.subject} ë²¡í„° DB êµ¬ì¶• ì™„ë£Œ!")
            print(f"ğŸ“Š ì»¬ë ‰ì…˜: {self.collection_name}")
            print(f"ğŸ“Š ë²¡í„° ê°œìˆ˜: {len(embedded_chunks)}")
            
        except Exception as e:
            print(f"âŒ ë²¡í„° DB êµ¬ì¶• ì‹¤íŒ¨: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="RAG ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•")
    parser.add_argument("--subject", required=True, help="ì£¼ì œëª… (ì˜ˆ: unitask)")
    
    args = parser.parse_args()
    
    # ë²¡í„° ë¹Œë” ìƒì„± ë° ì‹¤í–‰
    builder = VectorBuilder(args.subject)
    builder.build_vector_db()

if __name__ == "__main__":
    main()